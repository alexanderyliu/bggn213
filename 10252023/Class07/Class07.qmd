---
title: "Class 7: Machine Learning 1"
author: "Alexander Liu (PID: 69026918)"
format: pdf
---

# Clustering

We will start with k-mneans clustering, one of the most prevelent of all clustering methods.

To get started let's make some data up:

```{r}
hist(rnorm(10000, 3))
```
```{r}
tmp <- c(rnorm(30,3), rnorm(30, -3))
x <- cbind(tmp, rev(tmp))
x
plot(x)
```
The main function in R for K-means clustering is called `kmeans()`.


```{r}
k <-  kmeans(x, centers=2, nstart=20)
k
```

>Q1. How many points are in each cluster

```{r}
k$size
```

>Q2. The clustering result i.e. membership vector?

```{r}
k$cluster
```

>Q3. Cluster centers

```{r}
k$centers
```

>Q4. Make a plot of our data colored by clustering results with optionally the cluster centers shown.

```{r}
plot(x, col=k$cluster, pch=16)
points(k$centers, col="blue", pch=15, cex=2)

```

>Q5. Run kmeans again but cluter into 3 groups and plot the results like we did above.

```{r}
l <-  kmeans(x, centers=3, nstart=20)
plot(x, col=l$cluster, pch=16)
points(l$centers, col="blue", pch=15, cex=2)
```

K-means will always return a clustering result - even if there is no clear groupings.

#Hierarchical Clustering

main function: `hclust()`


```{r}
hc <- hclust( dist(x) )
hc
```

```{r}
plot(hc)
abline(h=8, col="red")
```

The function to get our clusters/groups from a hclust object is called `cutree()`

```{r}
grps <- cutree(hc, h=8)
grps
```


>Q. Plot our hclust results in terms of our data colored by cluster membership.

```{r}
plot(x, col=grps)
```

# Principal Component Analysis (PCA)


Class 7 Lab

>Q1. How many rows and columns are in your new data frame named x? What R functions could you use to answer this questions?

```{r}
url <- "https://tinyurl.com/UK-foods"
x <- read.csv(url)

dim(x)
```

```{r}
## Preview the first 6 rows
head(x)
```

```{r}
# Note how the minus indexing works
rownames(x) <- x[,1]
x <- x[,-1]
head(x)
```

```{r}
dim(x)
```

```{r}
x <- read.csv(url, row.names=1)
head(x)
```

>Q2. Which approach to solving the ‘row-names problem’ mentioned above do you prefer and why? Is one approach more robust than another under certain circumstances?

I prefer the latter (row.names=1). If you accidentally run the former code, it could overwrite your processed dataset.


```{r}
barplot(as.matrix(x), beside=T, col=rainbow(nrow(x)))
```

>Q3: Changing what optional argument in the above barplot() function results in the following plot?

```{r}
barplot(as.matrix(x), beside=F, col=rainbow(nrow(x)),)
```

>Q5: Generating all pairwise plots may help somewhat. Can you make sense of the following code and resulting figure? What does it mean if a given point lies on the diagonal for a given plot?

Lying on the diagonal means the value between two countries are similalr.  


```{r}
pairs(x, col=rainbow(17), pch=16)
```
>Q6. What is the main differences between N. Ireland and the other countries of the UK in terms of this data-set?

There are more plots that are not on the diagonal, which indicates N.Ireland is more dissimilar to other countries. 


PCA to the rescue

```{r}
# Use the prcomp() PCA function 
pca <- prcomp( t(x) )
summary(pca)
```

>Q7. Complete the code below to generate a plot of PC1 vs PC2. The second line adds text labels over the data points.

```{r}
# Plot PC1 vs PC2
plot(pca$x[,1], pca$x[,2], xlab="PC1", ylab="PC2", xlim=c(-270,500))
text(pca$x[,1], pca$x[,2], colnames(x))
```

>Q8. Customize your plot so that the colors of the country names match the colors in our UK and Ireland map and table at start of this document.

```{r}
colors=c("orange", "red", "blue", "green")
plot(pca$x[,1], pca$x[,2], xlab="PC1", ylab="PC2", xlim=c(-270,500))
text(pca$x[,1], pca$x[,2], colnames(x), col=colors)
```

```{r}
v <- round( pca$sdev^2/sum(pca$sdev^2) * 100 )
v
```

```{r}
## or the second row here...
z <- summary(pca)
z$importance
```


Digging deeper (variable loadings)

```{r}
## Lets focus on PC1 as it accounts for > 90% of variance 
par(mar=c(10, 3, 0.35, 0))
barplot( pca$rotation[,1], las=2 )
```

>Q9: Generate a similar ‘loadings plot’ for PC2. What two food groups feature prominantely and what does PC2 maninly tell us about?

```{r}
par(mar=c(10, 3, 0.35, 0))
barplot( pca$rotation[,2], las=2 )
```
Fresh_potatoes and Soft_drinks.
They are the main drivers to "push" Walse and Scotland to negative or positive side, respectively.


Using ggplot for these figures

```{r}
library(ggplot2)

df <- as.data.frame(pca$x)
df_lab <- tibble::rownames_to_column(df, "Country")

# Our first basic plot
ggplot(df_lab) + 
  aes(PC1, PC2, col=Country) + 
  geom_point()

```

```{r}
ggplot(df_lab) + 
  aes(PC1, PC2, col=Country, label=Country) + 
  geom_hline(yintercept = 0, col="gray") +
  geom_vline(xintercept = 0, col="gray") +
  geom_point(show.legend = FALSE) +
  geom_label(hjust=1, nudge_x = -10, show.legend = FALSE) +
  expand_limits(x = c(-300,500)) +
  xlab("PC1 (67.4%)") +
  ylab("PC2 (28%)") +
  theme_bw()
```

```{r}
ld <- as.data.frame(pca$rotation)
ld_lab <- tibble::rownames_to_column(ld, "Food")

ggplot(ld_lab) +
  aes(PC1, Food) +
  geom_col() 
```

```{r}
ggplot(ld_lab) +
  aes(PC1, reorder(Food, PC1), bg=PC1) +
  geom_col() + 
  xlab("PC1 Loadings/Contributions") +
  ylab("Food Group") +
  scale_fill_gradient2(low="purple", mid="gray", high="darkgreen", guide=NULL) +
  theme_bw()
```

```{r}
## The inbuilt biplot() can be useful for small datasets 
biplot(pca)
```


2. PCA of RNA-seq data

```{r}
url2 <- "https://tinyurl.com/expression-CSV"
rna.data <- read.csv(url2, row.names=1)
head(rna.data)
```

>Q10: How many genes and samples are in this data set?

```{r}
nrow(rna.data)
```
100 genes

```{r}
## Again we have to take the transpose of our data 
pca <- prcomp(t(rna.data), scale=TRUE)
 
## Simple un polished plot of pc1 and pc2
plot(pca$x[,1], pca$x[,2], xlab="PC1", ylab="PC2")
```

```{r}
summary(pca)
```

```{r}
plot(pca, main="Quick scree plot")
```

```{r}
## Variance captured per PC 
pca.var <- pca$sdev^2

## Percent variance is often more informative to look at 
pca.var.per <- round(pca.var/sum(pca.var)*100, 1)
pca.var.per
```

```{r}
barplot(pca.var.per, main="Scree Plot", 
        names.arg = paste0("PC", 1:10),
        xlab="Principal Component", ylab="Percent Variation")
```

```{r}
## A vector of colors for wt and ko samples
colvec <- colnames(rna.data)
colvec[grep("wt", colvec)] <- "red"
colvec[grep("ko", colvec)] <- "blue"

plot(pca$x[,1], pca$x[,2], col=colvec, pch=16,
     xlab=paste0("PC1 (", pca.var.per[1], "%)"),
     ylab=paste0("PC2 (", pca.var.per[2], "%)"))

text(pca$x[,1], pca$x[,2], labels = colnames(rna.data), pos=c(rep(4,5), rep(2,5)))
```

```{r}
library(ggplot2)

df <- as.data.frame(pca$x)

# Our first basic plot
ggplot(df) + 
  aes(PC1, PC2) + 
  geom_point()
```

```{r}
# Add a 'wt' and 'ko' "condition" column
df$samples <- colnames(rna.data) 
df$condition <- substr(colnames(rna.data),1,2)

p <- ggplot(df) + 
        aes(PC1, PC2, label=samples, col=condition) + 
        geom_label(show.legend = FALSE)
p
```

```{r}
p + labs(title="PCA of RNASeq Data",
       subtitle = "PC1 clealy seperates wild-type from knock-out samples",
       x=paste0("PC1 (", pca.var.per[1], "%)"),
       y=paste0("PC2 (", pca.var.per[2], "%)"),
       caption="Class example data") +
     theme_bw()
```

```{r}
loading_scores <- pca$rotation[,1]

## Find the top 10 measurements (genes) that contribute
## most to PC1 in either direction (+ or -)
gene_scores <- abs(loading_scores) 
gene_score_ranked <- sort(gene_scores, decreasing=TRUE)

## show the names of the top 10 genes
top_10_genes <- names(gene_score_ranked[1:10])
top_10_genes 
```

```{r}
sessionInfo()
```

